"""
AI Summarizer for WeChat chat logs
"""
from abc import ABC, abstractmethod
from typing import List, Dict
import logging
import os
from openai import OpenAI
import google.generativeai as genai
from .proxy_manager import ProxyManager



class BaseSummarizer(ABC):
    """AIæ€»ç»“å™¨åŸºç±»"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @abstractmethod
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """æ€»ç»“èŠå¤©è®°å½•"""
        pass
    
    @abstractmethod
    def test_connection(self) -> bool:
        """æµ‹è¯•AIæœåŠ¡è¿žæŽ¥"""
        pass
    
    def _format_messages(self, chat_logs: List[Dict]) -> str:
        """æ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯ - é€šç”¨å®žçŽ°"""
        messages = []
        for log in chat_logs:
            time_str = log.get('time', '')
            sender = log.get('senderName', 'æœªçŸ¥ç”¨æˆ·')
            content = log.get('content', '')
            msg_type = log.get('type', 0)
            contents = log.get('contents', {})
            
            # ç²¾ç®€æ—¶é—´æ ¼å¼ (ä¿ç•™æœˆæ—¥æ—¶åˆ†)
            if time_str:
                try:
                    # ä»Ž "2025-07-04T10:05:32+08:00" æå– "07-04 10:05"
                    date_part = time_str.split('T')[0].split('-')[1:]  # ['07', '04']
                    time_part = time_str.split('T')[1].split('+')[0][:5]  # '10:05'
                    time_str = f"{date_part[0]}-{date_part[1]} {time_part}"
                except:
                    time_str = time_str
            
            # å¤„ç†ä¸åŒç±»åž‹çš„æ¶ˆæ¯
            formatted_content = ""
            
            if msg_type == 1:  # æ–‡æœ¬æ¶ˆæ¯
                formatted_content = content
            elif msg_type == 3:  # å›¾ç‰‡æ¶ˆæ¯ - è·³è¿‡ä¸å¤„ç†
                continue
            elif msg_type == 49:  # é“¾æŽ¥/å¼•ç”¨æ¶ˆæ¯
                sub_type = log.get('subType', 0)
                if sub_type == 51:  # é“¾æŽ¥åˆ†äº«
                    title = contents.get('title', '')
                    url = contents.get('url', '')
                    formatted_content = f"[åˆ†äº«] {title}" + (f" ({url})" if url else "")
                elif sub_type == 57 and contents.get('refer'):  # å¼•ç”¨æ¶ˆæ¯
                    refer = contents['refer']
                    refer_sender = refer.get('senderName', 'æœªçŸ¥ç”¨æˆ·')
                    refer_content = refer.get('content', '')
                    
                    # å¤„ç†å¼•ç”¨æ¶ˆæ¯ï¼Œä¿ç•™å®Œæ•´å†…å®¹
                    formatted_content = f"{content}\n  â”” å›žå¤ {refer_sender}: {refer_content}"
                else:
                    formatted_content = content or "[å…¶ä»–æ¶ˆæ¯]"
            else:
                # å…¶ä»–ç±»åž‹æ¶ˆæ¯ï¼Œåªä¿ç•™æœ‰æ–‡æœ¬å†…å®¹çš„
                if content and content.strip():
                    formatted_content = content
                else:
                    continue  # è·³è¿‡æ— æ–‡æœ¬å†…å®¹çš„æ¶ˆæ¯
            
            # è·³è¿‡ç©ºæ¶ˆæ¯
            if not formatted_content.strip():
                continue
                
            messages.append(f"[{time_str}] {sender}: {formatted_content}")
        
        return "\n".join(messages)
    
    def _build_prompt(self, group_name: str, formatted_messages: str) -> str:
        """æž„å»ºAIæç¤ºè¯ - é€šç”¨å®žçŽ°ï¼Œå­ç±»å¯ä»¥é‡å†™"""
        return f"""
è¯·åˆ†æžç¾¤èŠ '{group_name}' çš„èŠå¤©è®°å½•ï¼Œé‡ç‚¹å…³æ³¨æ ¸å¿ƒè¯é¢˜ã€æœ‰ä»·å€¼çš„è§‚ç‚¹å’Œé‡è¦ä¿¡æ¯åˆ†äº«ã€‚å¿½ç•¥æ²¡æœ‰æ„ä¹‰çš„å›¾ç‰‡é“¾æŽ¥ã€è¡¨æƒ…ç¬¦å·ç­‰å†…å®¹ã€‚

èŠå¤©è®°å½•ï¼š
{formatted_messages}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æž„åŒ–æ€»ç»“ï¼Œç›´æŽ¥è¾“å‡ºå†…å®¹ï¼Œä¸è¦è¾“å‡ºè§£é‡Šæ€§æ–‡å­—ï¼š

## ðŸ“Š ç¾¤èŠæ¦‚å†µ
- **ç¾¤èŠåç§°**: {group_name}
- **æ´»è·ƒæˆå‘˜**: [ç»Ÿè®¡å‘è¨€äººæ•°]
- **æ¶ˆæ¯æ€»æ•°**: [ç»Ÿè®¡æœ‰æ•ˆæ¶ˆæ¯æ•°é‡]
- **æ—¶é—´è·¨åº¦**: [è®°å½•æ—¶é—´èŒƒå›´]

## ðŸ”¥ æ ¸å¿ƒè¯é¢˜
[æŒ‰é‡è¦æ€§æŽ’åºï¼Œåˆ—å‡ºæœ€å¤š 20 ä¸ªä¸»è¦è®¨è®ºè¯é¢˜ï¼Œæ¯ä¸ªè¯é¢˜åŒ…å«å…³é”®è§‚ç‚¹]

**è¯é¢˜ä¸€**: 
   - æ ¸å¿ƒå†…å®¹: 
   - ä¸»è¦è§‚ç‚¹: 
   - å‚ä¸Žè®¨è®º: 

**è¯é¢˜äºŒ**: 
   - æ ¸å¿ƒå†…å®¹: 
   - ä¸»è¦è§‚ç‚¹: 
   - å‚ä¸Žè®¨è®º: 

## ðŸ’¡ æœ‰ä»·å€¼ä¿¡æ¯
[æå–é‡è¦çš„ä¿¡æ¯åˆ†äº«ã€èµ„æºæŽ¨èã€ç»éªŒæ€»ç»“ç­‰]

- **é‡è¦é€šçŸ¥**: 
- **èµ„æºåˆ†äº«**: 
- **ç»éªŒåˆ†äº«**: 
- **å†³ç­–äº‹é¡¹**: 

## â“ FAQ å¸¸è§é—®é¢˜
[å°½å¯èƒ½å¤šçš„æ•´ç†ç¾¤å†…è®¨è®ºçš„æœ‰ä»·å€¼çš„é—®é¢˜å’Œè§£ç­”ï¼Œæœ€å¤š 20 ä¸ªé—®é¢˜]

**Q1**: [é—®é¢˜æè¿°]
**A1**: [è§£ç­”å†…å®¹]

**Q2**: [é—®é¢˜æè¿°]
**A2**: [è§£ç­”å†…å®¹]

## ðŸŽ¯ å¾…è·Ÿè¿›äº‹é¡¹
[éœ€è¦åŽç»­å…³æ³¨æˆ–è¡ŒåŠ¨çš„äº‹é¡¹]

- [ ] [å¾…åŠžäº‹é¡¹1]
- [ ] [å¾…åŠžäº‹é¡¹2]

## ðŸ“ å¤‡æ³¨
[å…¶ä»–å€¼å¾—å…³æ³¨çš„ä¿¡æ¯æˆ–è§‚å¯Ÿ]

---
*æœ¬æ€»ç»“åŸºäºŽAIåˆ†æžç”Ÿæˆï¼Œå¦‚æœ‰é—æ¼è¯·å‚è€ƒåŽŸå§‹èŠå¤©è®°å½•*
"""


class OpenAISummarizer(BaseSummarizer):
    """OpenAIæ€»ç»“å™¨"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini", proxy_manager: ProxyManager = None):
        super().__init__()
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.proxy_manager = proxy_manager or ProxyManager()
    
    def test_connection(self) -> bool:
        """æµ‹è¯•OpenAI APIè¿žæŽ¥"""
        try:
            with self.proxy_manager.proxy_context():
                # ä½¿ç”¨models APIæ¥æµ‹è¯•è¿žæŽ¥
                models = self.client.models.list()
                # æ£€æŸ¥æŒ‡å®šçš„æ¨¡åž‹æ˜¯å¦å¯ç”¨
                available_models = [model.id for model in models.data]
                if self.model in available_models:
                    self.logger.info(f"OpenAI API connection successful, model {self.model} is available")
                    return True
                else:
                    self.logger.warning(f"OpenAI API connected but model {self.model} not found in available models")
                    return False
        except Exception as e:
            self.logger.error(f"OpenAI API connection failed: {e}")
            return False
    
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """ä½¿ç”¨OpenAIæ€»ç»“èŠå¤©è®°å½•"""
        if not chat_logs:
            return f"ç¾¤èŠ '{group_name}' æš‚æ— èŠå¤©è®°å½•"
        
        # æ ¼å¼åŒ–èŠå¤©è®°å½•
        formatted_messages = self._format_messages(chat_logs)
        
        # æž„å»ºæç¤ºè¯
        prompt = self._build_prompt(group_name, formatted_messages)
        
        try:
            # ä½¿ç”¨ä»£ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with self.proxy_manager.proxy_context():
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŠå¤©è®°å½•åˆ†æžå¸ˆï¼Œå–„äºŽä»Žç¾¤èŠè®°å½•ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç®€æ´æœ‰ç”¨çš„æ€»ç»“ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3
                )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.logger.error(f"OpenAI API error: {e}")
            raise RuntimeError(f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")
    



class LocalSummarizer(BaseSummarizer):
    """æœ¬åœ°æ€»ç»“å™¨ï¼ˆç®€å•æ–‡æœ¬å¤„ç†ï¼‰"""
    
    def __init__(self):
        super().__init__()
    
    def test_connection(self) -> bool:
        """æµ‹è¯•æœ¬åœ°æ€»ç»“å™¨ï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰"""
        self.logger.info("Local summarizer is always available")
        return True
    
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """ä½¿ç”¨ç®€å•è§„åˆ™æ€»ç»“èŠå¤©è®°å½•"""
        if not chat_logs:
            return f"ç¾¤èŠ '{group_name}' æš‚æ— èŠå¤©è®°å½•"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_messages = len([log for log in chat_logs if log.get('type') == 1])
        senders = set()
        keywords = {}
        
        for log in chat_logs:
            if log.get('type') == 1:  # æ–‡æœ¬æ¶ˆæ¯
                sender = log.get('senderName', 'æœªçŸ¥')
                senders.add(sender)
                content = log.get('content', '').lower()
                
                # ç®€å•å…³é”®è¯ç»Ÿè®¡
                common_words = ['ä¼šè®®', 'æ—¶é—´', 'åœ°ç‚¹', 'æ˜Žå¤©', 'ä»Šå¤©', 'é¡¹ç›®', 'å·¥ä½œ']
                for word in common_words:
                    if word in content:
                        keywords[word] = keywords.get(word, 0) + 1
        
        # ç”Ÿæˆç®€å•æ€»ç»“
        summary = f"""## ç¾¤èŠæ€»ç»“ï¼š{group_name}

### åŸºæœ¬ä¿¡æ¯
- å‚ä¸Žäººæ•°ï¼š{len(senders)}äºº
- æ¶ˆæ¯æ€»æ•°ï¼š{total_messages}æ¡

### å‚ä¸Žæˆå‘˜
{', '.join(list(senders)[:10])}{'...' if len(senders) > 10 else ''}

### çƒ­é—¨å…³é”®è¯
{', '.join([f'{k}({v}æ¬¡)' for k, v in sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:5]])}

*æ³¨ï¼šè¿™æ˜¯ç®€å•ç»Ÿè®¡æ€»ç»“ï¼Œå¦‚éœ€è¯¦ç»†åˆ†æžè¯·é…ç½®AIæœåŠ¡*
"""
        return summary


class GeminiSummarizer(BaseSummarizer):
    """Google Geminiæ€»ç»“å™¨"""
    
    def __init__(self, api_key: str, model: str = "gemini-1.5-flash", proxy_manager: ProxyManager = None):
        super().__init__()
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model)
        self.model_name = model
        self.proxy_manager = proxy_manager or ProxyManager()
    
    def test_connection(self) -> bool:
        """æµ‹è¯•Gemini APIè¿žæŽ¥"""
        try:
            with self.proxy_manager.proxy_context():
                # ä½¿ç”¨list_models APIæ¥æµ‹è¯•è¿žæŽ¥
                available_models = []
                for model in genai.list_models():
                    if 'generateContent' in model.supported_generation_methods:
                        available_models.append(model.name)
                
                # æ£€æŸ¥æŒ‡å®šçš„æ¨¡åž‹æ˜¯å¦å¯ç”¨
                model_full_name = f"models/{self.model_name}"
                if model_full_name in available_models:
                    self.logger.info(f"Gemini API connection successful, model {self.model_name} is available")
                    return True
                else:
                    self.logger.warning(f"Gemini API connected but model {self.model_name} not found in available models")
                    return False
        except Exception as e:
            self.logger.error(f"Gemini API connection failed: {e}")
            return False
    
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """ä½¿ç”¨Google Geminiæ€»ç»“èŠå¤©è®°å½•"""
        if not chat_logs:
            return f"ç¾¤èŠ '{group_name}' æš‚æ— èŠå¤©è®°å½•"
        
        # æ ¼å¼åŒ–èŠå¤©è®°å½•
        formatted_messages = self._format_messages(chat_logs)
        
        # æž„å»ºæç¤ºè¯
        prompt = self._build_prompt(group_name, formatted_messages)
        
        try:
            # ä½¿ç”¨ä»£ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with self.proxy_manager.proxy_context():
                response = self.model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.3
                    )
                )
            return response.text.strip()
        except Exception as e:
            self.logger.error(f"Gemini API error: {e}")
            self.logger.exception(e)
            raise RuntimeError(f"Gemini APIè°ƒç”¨å¤±è´¥: {str(e)}")
    



class SummarizerFactory:
    """æ€»ç»“å™¨å·¥åŽ‚ç±»"""
    
    @staticmethod
    def create_summarizer(service_type: str, **kwargs) -> BaseSummarizer:
        """åˆ›å»ºæ€»ç»“å™¨å®žä¾‹"""
        proxy_manager = kwargs.get('proxy_manager')
        
        if service_type.lower() == "openai":
            api_key = kwargs.get('api_key') or os.getenv('OPENAI_API_KEY')
            if not api_key:
                raise ValueError("OpenAI API key is required")
            model = kwargs.get('model', 'gpt-4o-mini')
            return OpenAISummarizer(api_key=api_key, model=model, proxy_manager=proxy_manager)
        
        elif service_type.lower() == "gemini":
            api_key = kwargs.get('api_key') or os.getenv('GEMINI_API_KEY')
            if not api_key:
                raise ValueError("Gemini API key is required")
            model = kwargs.get('model', 'gemini-1.5-flash')
            return GeminiSummarizer(api_key=api_key, model=model, proxy_manager=proxy_manager)
        
        elif service_type.lower() == "local":
            return LocalSummarizer()
        
        else:
            raise ValueError(f"Unsupported AI service: {service_type}")