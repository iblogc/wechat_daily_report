"""
AI Summarizer for WeChat chat logs
"""
from abc import ABC, abstractmethod
from typing import List, Dict
import logging
import os
from openai import OpenAI
import google.generativeai as genai
from .proxy_manager import ProxyManager



class BaseSummarizer(ABC):
    """AIæ€»ç»“å™¨åŸºç±»"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @abstractmethod
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """æ€»ç»“èŠå¤©è®°å½•"""
        pass
    
    @abstractmethod
    def test_connection(self) -> bool:
        """æµ‹è¯•AIæœåŠ¡è¿æ¥"""
        pass
    
    def _format_messages(self, chat_logs: List[Dict]) -> str:
        """æ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯ - é€šç”¨å®ç°"""
        messages = []
        for log in chat_logs:
            time_str = log.get('time', '')
            sender = log.get('senderName', 'æœªçŸ¥ç”¨æˆ·')
            content = log.get('content', '')
            msg_type = log.get('type', 0)
            contents = log.get('contents', {})
            is_self = log.get('isSelf', False)
            
            # ç²¾ç®€æ—¶é—´æ ¼å¼ (ä¿ç•™æœˆæ—¥æ—¶åˆ†)
            if time_str:
                try:
                    # ä» "2025-07-04T10:05:32+08:00" æå– "07-04 10:05"
                    date_part = time_str.split('T')[0].split('-')[1:]  # ['07', '04']
                    time_part = time_str.split('T')[1].split('+')[0][:5]  # '10:05'
                    time_str = f"{date_part[0]}-{date_part[1]} {time_part}"
                except:
                    time_str = time_str
            
            # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
            formatted_content = ""
            
            if msg_type == 1:  # æ–‡æœ¬æ¶ˆæ¯
                formatted_content = content
            elif msg_type == 3:  # å›¾ç‰‡æ¶ˆæ¯ - è·³è¿‡ä¸å¤„ç†
                continue
            elif msg_type == 49:  # é“¾æ¥/å¼•ç”¨æ¶ˆæ¯
                sub_type = log.get('subType', 0)
                if sub_type == 51:  # é“¾æ¥åˆ†äº«
                    title = contents.get('title', '')
                    url = contents.get('url', '')
                    formatted_content = f"[åˆ†äº«] [{title}]" + (f"({url})" if url else "")
                elif sub_type == 57 and contents.get('refer'):  # å¼•ç”¨æ¶ˆæ¯
                    refer = contents['refer']
                    refer_sender = refer.get('senderName', 'æœªçŸ¥ç”¨æˆ·')
                    refer_content = refer.get('content', '')
                    refer_is_self = refer.get('isSelf', False)
                    
                    # ä¸ºå¼•ç”¨æ¶ˆæ¯çš„å‘é€è€…ä¹Ÿæ·»åŠ ç‰¹æ®Šæ ‡è®°
                    refer_sender_display = f"{refer_sender} [æˆ‘]" if refer_is_self else refer_sender
                    
                    # å¤„ç†å¼•ç”¨æ¶ˆæ¯ï¼Œä¿ç•™å®Œæ•´å†…å®¹
                    formatted_content = f"{content}\n  â”” å›å¤ {refer_sender_display}: {refer_content}"
                else:
                    formatted_content = content or "[å…¶ä»–æ¶ˆæ¯]"
            else:
                # å…¶ä»–ç±»å‹æ¶ˆæ¯ï¼Œåªä¿ç•™æœ‰æ–‡æœ¬å†…å®¹çš„
                if content and content.strip():
                    formatted_content = content
                else:
                    continue  # è·³è¿‡æ— æ–‡æœ¬å†…å®¹çš„æ¶ˆæ¯
            
            # è·³è¿‡ç©ºæ¶ˆæ¯
            if not formatted_content.strip():
                continue
            
            # ä¸ºè‡ªå·±å‘é€çš„æ¶ˆæ¯æ·»åŠ ç‰¹æ®Šæ ‡è®°
            sender_display = f"{sender} [æˆ‘]" if is_self else sender
            messages.append(f"[{time_str}] {sender_display}: {formatted_content}")
        
        return "\n".join(messages)
    
    def _build_prompt(self, group_name: str, formatted_messages: str) -> str:
        """æ„å»ºAIæç¤ºè¯ - é€šç”¨å®ç°ï¼Œå­ç±»å¯ä»¥é‡å†™"""
        return f"""
è¯·åˆ†æç¾¤èŠ '{group_name}' çš„èŠå¤©è®°å½•ï¼Œé‡ç‚¹å…³æ³¨æ ¸å¿ƒè¯é¢˜ã€æœ‰ä»·å€¼çš„è§‚ç‚¹å’Œé‡è¦ä¿¡æ¯åˆ†äº«ã€‚å¿½ç•¥æ²¡æœ‰æ„ä¹‰çš„å›¾ç‰‡é“¾æ¥ã€è¡¨æƒ…ç¬¦å·ç­‰å†…å®¹ã€‚
åˆ†ææ€»ç»“ç›®çš„æ˜¯ä¸ºäº†è®©é˜…è¯»è€…é«˜æ•ˆå¿«é€Ÿçš„ä»ç¾¤èŠä¸­å¸å–å­¦ä¹ äººä»·å€¼çš„å†…å®¹ã€‚
æŒ‡å¯¼åŸåˆ™ï¼š
1.  **ç²¾å‡†æç‚¼**: èšç„¦äºçœŸæ­£æœ‰ä»·å€¼çš„è®¨è®ºã€ä¿¡æ¯å’Œå†³ç­–ï¼Œå¿½ç•¥é—²èŠã€è¡¨æƒ…ã€é‡å¤ç¡®è®¤ç­‰å™ªéŸ³ã€‚
2.  **å®¢è§‚ä¸­ç«‹**: ä¸¥æ ¼åŸºäºèŠå¤©è®°å½•è¿›è¡Œæ€»ç»“ï¼Œä¸æ·»åŠ ä»»ä½•ä¸»è§‚è‡†æ–­æˆ–å¤–éƒ¨ä¿¡æ¯ã€‚
3.  **ç»“æ„æ¸…æ™°**: ä¸¥æ ¼æŒ‰ç…§ä¸‹æ–¹æŒ‡å®šçš„ Markdown æ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æŠ¥å‘Šçš„ä¸“ä¸šæ€§å’Œå¯è¯»æ€§ã€‚
4.  **ä¿¡æ¯å®Œæ•´**: å¦‚æœæ˜¯ä¸€äº›æœ‰ä»·å€¼ã€æ•™å­¦ç±»çš„å†…å®¹ï¼Œè¦æŠŠå®Œæ•´å†…å®¹æå–ï¼Œä¼˜åŒ–è¡¨è¾¾åå®Œæ•´ä¿ç•™ã€‚

èŠå¤©è®°å½•ï¼š
{formatted_messages}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æ„åŒ–æ€»ç»“ï¼Œæ³¨æ„å†…å®¹å±‚çº§ç¼©è¿›ï¼Œä½¿ç”¨æ ‡å‡†çš„markdownè¯­æ³•ï¼ˆCommonMarkï¼‰ï¼Œç›´æ¥è¾“å‡ºå†…å®¹ï¼Œä¸è¦è¾“å‡ºè§£é‡Šæ€§æ–‡å­—ï¼š

## ğŸ“Š ç¾¤èŠæ¦‚å†µ
- **ç¾¤èŠåç§°**: {group_name}
- **æ´»è·ƒæˆå‘˜**: [ç»Ÿè®¡å‘è¨€äººæ•°]
- **æ¶ˆæ¯æ€»æ•°**: [ç»Ÿè®¡æœ‰æ•ˆæ¶ˆæ¯æ•°é‡]
- **æ—¶é—´è·¨åº¦**: [è®°å½•æ—¶é—´èŒƒå›´]

## ğŸ”¥ æ ¸å¿ƒè¯é¢˜
[æŒ‰é‡è¦æ€§æ’åºï¼Œåˆ—å‡ºæœ€å¤š 20 ä¸ªä¸»è¦è®¨è®ºè¯é¢˜ï¼Œæ¯ä¸ªè¯é¢˜åŒ…å«å…³é”®è§‚ç‚¹]

**è¯é¢˜ä¸€**: 

   - æ ¸å¿ƒå†…å®¹: 
   - ä¸»è¦è§‚ç‚¹: 
   - å‚ä¸è®¨è®º: 

**è¯é¢˜äºŒ**: 

   - æ ¸å¿ƒå†…å®¹: 
   - ä¸»è¦è§‚ç‚¹: 
   - å‚ä¸è®¨è®º: 

## ğŸ’¡ æœ‰ä»·å€¼ä¿¡æ¯
[æå–é‡è¦çš„ä¿¡æ¯åˆ†äº«ã€èµ„æºæ¨èã€ç»éªŒæ€»ç»“ç­‰]

- **é‡è¦é€šçŸ¥**: 
- **èµ„æºåˆ†äº«**: 
- **ç»éªŒåˆ†äº«**: 
- **å†³ç­–äº‹é¡¹**: 

## â“ FAQ å¸¸è§é—®é¢˜
[å°½å¯èƒ½å¤šçš„æ•´ç†ç¾¤å†…è®¨è®ºçš„æœ‰ä»·å€¼çš„é—®é¢˜å’Œè§£ç­”ï¼Œæœ€å¤š 20 ä¸ªé—®é¢˜]

**Q1**: [é—®é¢˜æè¿°]
**A1**: [è§£ç­”å†…å®¹]

**Q2**: [é—®é¢˜æè¿°]
**A2**: [è§£ç­”å†…å®¹]

## ğŸ“ å¤‡æ³¨
[å…¶ä»–å€¼å¾—å…³æ³¨çš„ä¿¡æ¯æˆ–è§‚å¯Ÿ]

---
*æœ¬æ€»ç»“åŸºäºAIåˆ†æç”Ÿæˆï¼Œå¦‚æœ‰é—æ¼è¯·å‚è€ƒåŸå§‹èŠå¤©è®°å½•*
"""


class OpenAISummarizer(BaseSummarizer):
    """OpenAIæ€»ç»“å™¨"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini", proxy_manager: ProxyManager = None):
        super().__init__()
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.proxy_manager = proxy_manager or ProxyManager()
    
    def test_connection(self) -> bool:
        """æµ‹è¯•OpenAI APIè¿æ¥"""
        try:
            with self.proxy_manager.proxy_context():
                # ä½¿ç”¨models APIæ¥æµ‹è¯•è¿æ¥
                models = self.client.models.list()
                # æ£€æŸ¥æŒ‡å®šçš„æ¨¡å‹æ˜¯å¦å¯ç”¨
                available_models = [model.id for model in models.data]
                if self.model in available_models:
                    self.logger.info(f"OpenAI API connection successful, model {self.model} is available")
                    return True
                else:
                    self.logger.warning(f"OpenAI API connected but model {self.model} not found in available models")
                    return False
        except Exception as e:
            self.logger.error(f"OpenAI API connection failed: {e}")
            return False
    
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """ä½¿ç”¨OpenAIæ€»ç»“èŠå¤©è®°å½•"""
        if not chat_logs:
            return f"ç¾¤èŠ '{group_name}' æš‚æ— èŠå¤©è®°å½•"
        
        # æ ¼å¼åŒ–èŠå¤©è®°å½•
        formatted_messages = self._format_messages(chat_logs)
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(group_name, formatted_messages)
        
        try:
            # ä½¿ç”¨ä»£ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with self.proxy_manager.proxy_context():
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŠå¤©è®°å½•åˆ†æå¸ˆï¼Œå–„äºä»ç¾¤èŠè®°å½•ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç®€æ´æœ‰ç”¨çš„æ€»ç»“ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3
                )
            
            # è·å–AIç”Ÿæˆçš„æ€»ç»“
            ai_summary = response.choices[0].message.content.strip()
            
            # æ·»åŠ æŠ˜å çš„å®Œæ•´èŠå¤©è®°å½•
            collapsible_section = f"""

<details>
<summary>ğŸ“‹ å®Œæ•´èŠå¤©è®°å½•</summary>

```
{formatted_messages}
```

</details>"""
            
            return ai_summary + collapsible_section
            
        except Exception as e:
            self.logger.error(f"OpenAI API error: {e}")
            raise RuntimeError(f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")
    



class LocalSummarizer(BaseSummarizer):
    """æœ¬åœ°æ€»ç»“å™¨ï¼ˆç®€å•æ–‡æœ¬å¤„ç†ï¼‰"""
    
    def __init__(self):
        super().__init__()
    
    def test_connection(self) -> bool:
        """æµ‹è¯•æœ¬åœ°æ€»ç»“å™¨ï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰"""
        self.logger.info("Local summarizer is always available")
        return True
    
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """ä½¿ç”¨ç®€å•è§„åˆ™æ€»ç»“èŠå¤©è®°å½•"""
        if not chat_logs:
            return f"ç¾¤èŠ '{group_name}' æš‚æ— èŠå¤©è®°å½•"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_messages = len([log for log in chat_logs if log.get('type') == 1])
        senders = set()
        keywords = {}
        
        for log in chat_logs:
            if log.get('type') == 1:  # æ–‡æœ¬æ¶ˆæ¯
                sender = log.get('senderName', 'æœªçŸ¥')
                senders.add(sender)
                content = log.get('content', '').lower()
                
                # ç®€å•å…³é”®è¯ç»Ÿè®¡
                common_words = ['ä¼šè®®', 'æ—¶é—´', 'åœ°ç‚¹', 'æ˜å¤©', 'ä»Šå¤©', 'é¡¹ç›®', 'å·¥ä½œ']
                for word in common_words:
                    if word in content:
                        keywords[word] = keywords.get(word, 0) + 1
        
        # æ ¼å¼åŒ–èŠå¤©è®°å½•
        formatted_messages = self._format_messages(chat_logs)
        
        # ç”Ÿæˆç®€å•æ€»ç»“
        summary = f"""## ç¾¤èŠæ€»ç»“ï¼š{group_name}

### åŸºæœ¬ä¿¡æ¯
- å‚ä¸äººæ•°ï¼š{len(senders)}äºº
- æ¶ˆæ¯æ€»æ•°ï¼š{total_messages}æ¡

### å‚ä¸æˆå‘˜
{', '.join(list(senders)[:10])}{'...' if len(senders) > 10 else ''}

### çƒ­é—¨å…³é”®è¯
{', '.join([f'{k}({v}æ¬¡)' for k, v in sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:5]])}

*æ³¨ï¼šè¿™æ˜¯ç®€å•ç»Ÿè®¡æ€»ç»“ï¼Œå¦‚éœ€è¯¦ç»†åˆ†æè¯·é…ç½®AIæœåŠ¡*
"""
        
        # æ·»åŠ æŠ˜å çš„å®Œæ•´èŠå¤©è®°å½•
        collapsible_section = f"""

<details>
<summary>ğŸ“‹ å®Œæ•´èŠå¤©è®°å½•</summary>

```
{formatted_messages}
```

</details>"""
        
        return summary + collapsible_section


class GeminiSummarizer(BaseSummarizer):
    """Google Geminiæ€»ç»“å™¨"""
    
    def __init__(self, api_key: str, model: str = "gemini-1.5-flash", proxy_manager: ProxyManager = None):
        super().__init__()
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model)
        self.model_name = model
        self.proxy_manager = proxy_manager or ProxyManager()
    
    def test_connection(self) -> bool:
        """æµ‹è¯•Gemini APIè¿æ¥"""
        try:
            with self.proxy_manager.proxy_context():
                # ä½¿ç”¨list_models APIæ¥æµ‹è¯•è¿æ¥
                available_models = []
                for model in genai.list_models():
                    if 'generateContent' in model.supported_generation_methods:
                        available_models.append(model.name)
                
                # æ£€æŸ¥æŒ‡å®šçš„æ¨¡å‹æ˜¯å¦å¯ç”¨
                model_full_name = f"models/{self.model_name}"
                if model_full_name in available_models:
                    self.logger.info(f"Gemini API connection successful, model {self.model_name} is available")
                    return True
                else:
                    self.logger.warning(f"Gemini API connected but model {self.model_name} not found in available models")
                    return False
        except Exception as e:
            self.logger.error(f"Gemini API connection failed: {e}")
            return False
    
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """ä½¿ç”¨Google Geminiæ€»ç»“èŠå¤©è®°å½•"""
        if not chat_logs:
            return f"ç¾¤èŠ '{group_name}' æš‚æ— èŠå¤©è®°å½•"
        
        # æ ¼å¼åŒ–èŠå¤©è®°å½•
        formatted_messages = self._format_messages(chat_logs)
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(group_name, formatted_messages)
        
        try:
            # ä½¿ç”¨ä»£ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with self.proxy_manager.proxy_context():
                response = self.model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.3
                    )
                )
            
            # è·å–AIç”Ÿæˆçš„æ€»ç»“
            ai_summary = response.text.strip()
            
            # æ·»åŠ æŠ˜å çš„å®Œæ•´èŠå¤©è®°å½•
            collapsible_section = f"""

<details>
<summary>ğŸ“‹ å®Œæ•´èŠå¤©è®°å½•</summary>

```
{formatted_messages}
```

</details>"""
            
            return ai_summary + collapsible_section
            
        except Exception as e:
            self.logger.error(f"Gemini API error: {e}")
            self.logger.exception(e)
            raise RuntimeError(f"Gemini APIè°ƒç”¨å¤±è´¥: {str(e)}")
    



class SummarizerFactory:
    """æ€»ç»“å™¨å·¥å‚ç±»"""
    
    @staticmethod
    def create_summarizer(service_type: str, **kwargs) -> BaseSummarizer:
        """åˆ›å»ºæ€»ç»“å™¨å®ä¾‹"""
        proxy_manager = kwargs.get('proxy_manager')
        
        if service_type.lower() == "openai":
            api_key = kwargs.get('api_key') or os.getenv('OPENAI_API_KEY')
            if not api_key:
                raise ValueError("OpenAI API key is required")
            model = kwargs.get('model', 'gpt-4o-mini')
            return OpenAISummarizer(api_key=api_key, model=model, proxy_manager=proxy_manager)
        
        elif service_type.lower() == "gemini":
            api_key = kwargs.get('api_key') or os.getenv('GEMINI_API_KEY')
            if not api_key:
                raise ValueError("Gemini API key is required")
            model = kwargs.get('model', 'gemini-1.5-flash')
            return GeminiSummarizer(api_key=api_key, model=model, proxy_manager=proxy_manager)
        
        elif service_type.lower() == "local":
            return LocalSummarizer()
        
        else:
            raise ValueError(f"Unsupported AI service: {service_type}")