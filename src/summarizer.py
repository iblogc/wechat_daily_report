"""
AI Summarizer for WeChat chat logs
"""
from abc import ABC, abstractmethod
from typing import List, Dict
import logging
import os
from openai import OpenAI
import google.generativeai as genai



class BaseSummarizer(ABC):
    """AIæ€»ç»“å™¨åŸºç±»"""
    
    @abstractmethod
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """æ€»ç»“èŠå¤©è®°å½•"""
        pass


class OpenAISummarizer(BaseSummarizer):
    """OpenAIæ€»ç»“å™¨"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.logger = logging.getLogger(__name__)
    
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """ä½¿ç”¨OpenAIæ€»ç»“èŠå¤©è®°å½•"""
        if not chat_logs:
            return f"ç¾¤èŠ '{group_name}' æš‚æ— èŠå¤©è®°å½•"
        
        # æ ¼å¼åŒ–èŠå¤©è®°å½•
        formatted_messages = self._format_messages(chat_logs)
        
        # æž„å»ºæç¤ºè¯
        prompt = self._build_prompt(group_name, formatted_messages)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŠå¤©è®°å½•åˆ†æžå¸ˆï¼Œå–„äºŽä»Žç¾¤èŠè®°å½•ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç®€æ´æœ‰ç”¨çš„æ€»ç»“ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            self.logger.error(f"OpenAI API error: {e}")
            raise RuntimeError(f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _format_messages(self, chat_logs: List[Dict]) -> str:
        """æ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯"""
        messages = []
        for log in chat_logs:
            time_str = log.get('time', '')
            sender = log.get('senderName', 'æœªçŸ¥ç”¨æˆ·')
            content = log.get('content', '')
            
            # è·³è¿‡ç©ºæ¶ˆæ¯æˆ–ç³»ç»Ÿæ¶ˆæ¯
            if not content or log.get('type') != 1:
                continue
                
            messages.append(f"[{time_str}] {sender}: {content}")
        
        return "\n".join(messages[-50:])  # åªå–æœ€è¿‘50æ¡æ¶ˆæ¯é¿å…tokenè¶…é™
    
    def _build_prompt(self, group_name: str, formatted_messages: str) -> str:
        """æž„å»ºAIæç¤ºè¯"""
        return f"""
è¯·åˆ†æžç¾¤èŠ '{group_name}' çš„èŠå¤©è®°å½•ï¼Œé‡ç‚¹å…³æ³¨æ ¸å¿ƒè¯é¢˜ã€æœ‰ä»·å€¼çš„è§‚ç‚¹å’Œé‡è¦ä¿¡æ¯åˆ†äº«ã€‚å¿½ç•¥æ²¡æœ‰æ„ä¹‰çš„å›¾ç‰‡é“¾æŽ¥ã€è¡¨æƒ…ç¬¦å·ç­‰å†…å®¹ã€‚

èŠå¤©è®°å½•ï¼š
{formatted_messages}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æž„åŒ–æ€»ç»“ï¼š

## ðŸ“Š ç¾¤èŠæ¦‚å†µ
- **ç¾¤èŠåç§°**: {group_name}
- **æ´»è·ƒæˆå‘˜**: [ç»Ÿè®¡å‘è¨€äººæ•°]
- **æ¶ˆæ¯æ€»æ•°**: [ç»Ÿè®¡æœ‰æ•ˆæ¶ˆæ¯æ•°é‡]
- **æ—¶é—´è·¨åº¦**: [è®°å½•æ—¶é—´èŒƒå›´]

## ðŸ”¥ æ ¸å¿ƒè¯é¢˜
[æŒ‰é‡è¦æ€§æŽ’åºï¼Œåˆ—å‡ºæœ€å¤š 20 ä¸ªä¸»è¦è®¨è®ºè¯é¢˜ï¼Œæ¯ä¸ªè¯é¢˜åŒ…å«å…³é”®è§‚ç‚¹]

1. **è¯é¢˜ä¸€**: 
   - æ ¸å¿ƒå†…å®¹: 
   - ä¸»è¦è§‚ç‚¹: 
   - å‚ä¸Žè®¨è®º: 

2. **è¯é¢˜äºŒ**: 
   - æ ¸å¿ƒå†…å®¹: 
   - ä¸»è¦è§‚ç‚¹: 
   - å‚ä¸Žè®¨è®º: 

## ðŸ’¡ æœ‰ä»·å€¼ä¿¡æ¯
[æå–é‡è¦çš„ä¿¡æ¯åˆ†äº«ã€èµ„æºæŽ¨èã€ç»éªŒæ€»ç»“ç­‰]

- **é‡è¦é€šçŸ¥**: 
- **èµ„æºåˆ†äº«**: 
- **ç»éªŒåˆ†äº«**: 
- **å†³ç­–äº‹é¡¹**: 

## â“ FAQ å¸¸è§é—®é¢˜
[æ•´ç†ç¾¤å†…è®¨è®ºçš„é—®é¢˜å’Œè§£ç­”ï¼Œæœ€å¤š 20 ä¸ªé—®é¢˜]

**Q1**: [é—®é¢˜æè¿°]
**A1**: [è§£ç­”å†…å®¹]

**Q2**: [é—®é¢˜æè¿°] 
**A2**: [è§£ç­”å†…å®¹]

## ðŸŽ¯ å¾…è·Ÿè¿›äº‹é¡¹
[éœ€è¦åŽç»­å…³æ³¨æˆ–è¡ŒåŠ¨çš„äº‹é¡¹]

- [ ] [å¾…åŠžäº‹é¡¹1]
- [ ] [å¾…åŠžäº‹é¡¹2]

## ðŸ“ å¤‡æ³¨
[å…¶ä»–å€¼å¾—å…³æ³¨çš„ä¿¡æ¯æˆ–è§‚å¯Ÿ]

---
*æœ¬æ€»ç»“åŸºäºŽAIåˆ†æžç”Ÿæˆï¼Œå¦‚æœ‰é—æ¼è¯·å‚è€ƒåŽŸå§‹èŠå¤©è®°å½• by jovix*
"""


class LocalSummarizer(BaseSummarizer):
    """æœ¬åœ°æ€»ç»“å™¨ï¼ˆç®€å•æ–‡æœ¬å¤„ç†ï¼‰"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """ä½¿ç”¨ç®€å•è§„åˆ™æ€»ç»“èŠå¤©è®°å½•"""
        if not chat_logs:
            return f"ç¾¤èŠ '{group_name}' æš‚æ— èŠå¤©è®°å½•"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_messages = len([log for log in chat_logs if log.get('type') == 1])
        senders = set()
        keywords = {}
        
        for log in chat_logs:
            if log.get('type') == 1:  # æ–‡æœ¬æ¶ˆæ¯
                sender = log.get('senderName', 'æœªçŸ¥')
                senders.add(sender)
                content = log.get('content', '').lower()
                
                # ç®€å•å…³é”®è¯ç»Ÿè®¡
                common_words = ['ä¼šè®®', 'æ—¶é—´', 'åœ°ç‚¹', 'æ˜Žå¤©', 'ä»Šå¤©', 'é¡¹ç›®', 'å·¥ä½œ']
                for word in common_words:
                    if word in content:
                        keywords[word] = keywords.get(word, 0) + 1
        
        # ç”Ÿæˆç®€å•æ€»ç»“
        summary = f"""## ç¾¤èŠæ€»ç»“ï¼š{group_name}

### åŸºæœ¬ä¿¡æ¯
- å‚ä¸Žäººæ•°ï¼š{len(senders)}äºº
- æ¶ˆæ¯æ€»æ•°ï¼š{total_messages}æ¡

### å‚ä¸Žæˆå‘˜
{', '.join(list(senders)[:10])}{'...' if len(senders) > 10 else ''}

### çƒ­é—¨å…³é”®è¯
{', '.join([f'{k}({v}æ¬¡)' for k, v in sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:5]])}

*æ³¨ï¼šè¿™æ˜¯ç®€å•ç»Ÿè®¡æ€»ç»“ï¼Œå¦‚éœ€è¯¦ç»†åˆ†æžè¯·é…ç½®AIæœåŠ¡*
"""
        return summary


class GeminiSummarizer(BaseSummarizer):
    """Google Geminiæ€»ç»“å™¨"""
    
    def __init__(self, api_key: str, model: str = "gemini-1.5-flash"):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model)
        self.model_name = model
        self.logger = logging.getLogger(__name__)
    
    def summarize_chat_logs(self, chat_logs: List[Dict], group_name: str) -> str:
        """ä½¿ç”¨Google Geminiæ€»ç»“èŠå¤©è®°å½•"""
        if not chat_logs:
            return f"ç¾¤èŠ '{group_name}' æš‚æ— èŠå¤©è®°å½•"
        
        # æ ¼å¼åŒ–èŠå¤©è®°å½•
        formatted_messages = self._format_messages(chat_logs)
        
        # æž„å»ºæç¤ºè¯
        prompt = self._build_prompt(group_name, formatted_messages)
        
        try:
            response = self.model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=1000,
                )
            )
            return response.text.strip()
        except Exception as e:
            self.logger.error(f"Gemini API error: {e}")
            self.logger.exception(e)
            raise RuntimeError(f"Gemini APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    def _format_messages(self, chat_logs: List[Dict]) -> str:
        """æ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯"""
        messages = []
        for log in chat_logs:
            time_str = log.get('time', '')
            sender = log.get('senderName', 'æœªçŸ¥ç”¨æˆ·')
            content = log.get('content', '')
            
            # è·³è¿‡ç©ºæ¶ˆæ¯æˆ–ç³»ç»Ÿæ¶ˆæ¯
            if not content or log.get('type') != 1:
                continue
                
            messages.append(f"[{time_str}] {sender}: {content}")
        
        return "\n".join(messages[-50:])  # åªå–æœ€è¿‘50æ¡æ¶ˆæ¯é¿å…tokenè¶…é™
    
    def _build_prompt(self, group_name: str, formatted_messages: str) -> str:
        """æž„å»ºAIæç¤ºè¯"""
        return f"""
è¯·åˆ†æžç¾¤èŠ '{group_name}' çš„èŠå¤©è®°å½•ï¼Œé‡ç‚¹å…³æ³¨æ ¸å¿ƒè¯é¢˜ã€æœ‰ä»·å€¼çš„è§‚ç‚¹å’Œé‡è¦ä¿¡æ¯åˆ†äº«ã€‚å¿½ç•¥æ²¡æœ‰æ„ä¹‰çš„å›¾ç‰‡é“¾æŽ¥ã€è¡¨æƒ…ç¬¦å·ç­‰å†…å®¹ã€‚

èŠå¤©è®°å½•ï¼š
{formatted_messages}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æž„åŒ–æ€»ç»“ï¼š

## ðŸ“Š ç¾¤èŠæ¦‚å†µ
- **ç¾¤èŠåç§°**: {group_name}
- **æ´»è·ƒæˆå‘˜**: [ç»Ÿè®¡å‘è¨€äººæ•°]
- **æ¶ˆæ¯æ€»æ•°**: [ç»Ÿè®¡æœ‰æ•ˆæ¶ˆæ¯æ•°é‡]
- **æ—¶é—´è·¨åº¦**: [è®°å½•æ—¶é—´èŒƒå›´]

## ðŸ”¥ æ ¸å¿ƒè¯é¢˜
[æŒ‰é‡è¦æ€§æŽ’åºï¼Œåˆ—å‡º3-5ä¸ªä¸»è¦è®¨è®ºè¯é¢˜ï¼Œæ¯ä¸ªè¯é¢˜åŒ…å«å…³é”®è§‚ç‚¹]

1. **è¯é¢˜ä¸€**: 
   - æ ¸å¿ƒå†…å®¹: 
   - ä¸»è¦è§‚ç‚¹: 
   - å‚ä¸Žè®¨è®º: 

2. **è¯é¢˜äºŒ**: 
   - æ ¸å¿ƒå†…å®¹: 
   - ä¸»è¦è§‚ç‚¹: 
   - å‚ä¸Žè®¨è®º: 

## ðŸ’¡ æœ‰ä»·å€¼ä¿¡æ¯
[æå–é‡è¦çš„ä¿¡æ¯åˆ†äº«ã€èµ„æºæŽ¨èã€ç»éªŒæ€»ç»“ç­‰]

- **é‡è¦é€šçŸ¥**: 
- **èµ„æºåˆ†äº«**: 
- **ç»éªŒåˆ†äº«**: 
- **å†³ç­–äº‹é¡¹**: 

## â“ FAQ å¸¸è§é—®é¢˜
[å°½å¯èƒ½å¤šçš„æ•´ç†ç¾¤å†…è®¨è®ºçš„æœ‰ä»·å€¼çš„é—®é¢˜å’Œè§£ç­”]

**Q1**: [é—®é¢˜æè¿°]
**A1**: [è§£ç­”å†…å®¹]

**Q2**: [é—®é¢˜æè¿°] 
**A2**: [è§£ç­”å†…å®¹]

## ðŸŽ¯ å¾…è·Ÿè¿›äº‹é¡¹
[éœ€è¦åŽç»­å…³æ³¨æˆ–è¡ŒåŠ¨çš„äº‹é¡¹]

- [ ] [å¾…åŠžäº‹é¡¹1]
- [ ] [å¾…åŠžäº‹é¡¹2]

## ðŸ“ å¤‡æ³¨
[å…¶ä»–å€¼å¾—å…³æ³¨çš„ä¿¡æ¯æˆ–è§‚å¯Ÿ]

---
*æœ¬æ€»ç»“åŸºäºŽAIåˆ†æžç”Ÿæˆï¼Œå¦‚æœ‰é—æ¼è¯·å‚è€ƒåŽŸå§‹èŠå¤©è®°å½•*
"""


class SummarizerFactory:
    """æ€»ç»“å™¨å·¥åŽ‚ç±»"""
    
    @staticmethod
    def create_summarizer(service_type: str, **kwargs) -> BaseSummarizer:
        """åˆ›å»ºæ€»ç»“å™¨å®žä¾‹"""
        if service_type.lower() == "openai":
            api_key = kwargs.get('api_key') or os.getenv('OPENAI_API_KEY')
            if not api_key:
                raise ValueError("OpenAI API key is required")
            model = kwargs.get('model', 'gpt-4o-mini')
            return OpenAISummarizer(api_key=api_key, model=model)
        
        elif service_type.lower() == "gemini":
            api_key = kwargs.get('api_key') or os.getenv('GEMINI_API_KEY')
            if not api_key:
                raise ValueError("Gemini API key is required")
            model = kwargs.get('model', 'gemini-1.5-flash')
            return GeminiSummarizer(api_key=api_key, model=model)
        
        elif service_type.lower() == "local":
            return LocalSummarizer()
        
        else:
            raise ValueError(f"Unsupported AI service: {service_type}")